{
pi[c] = runif(1,0,1)
c=c+1
}
pi[C] = 1-sum(pi)
for(c in 1:C)
{
mu[c] = runif(1,min(x),max(x))
sigma2[c] = runif(1,1,10)
}
current = c(pi,mu,sigma2)
tol=1e-8
diff=100
iter=0
while(diff > tol)
{
previous = current
iter=iter+1
Initial = gammaick(x,C,mu,sigma2,pi)
mu = colSums((Initial)*x)/colSums(Initial)
pi = colMeans(Initial)
for(c in 1:C)
{
sigma2[c] = (sum((Initial[,c])*(x-mu[c])^2))/sum(Initial[,c])
}
current = c(pi,mu,sigma2)
diff = norm(previous-current,"2")
}
p.est = current
pi.est = p.est[1:C]
mu.est = p.est[(C+1):(2*C)]
sigma2.est = p.est[(2*C+1):(3*C)]
z.est = gammaick(x,C,mu.est,sigma2.est,pi.est)
return=list(z.est,mu.est,sigma2.est,pi.est)
}
erup = GMMoneDim(faithful$eruptions,C=3)
# function that calculates the
# the log_likelihood
# for this multivariate setup
library(mvtnorm)
log_like <- function(X, pi.list, mu.list, Sigma.list, C)
{
foo <- 0
for(c in 1:C)
{
foo <- foo + pi.list[[c]]*dmvnorm(X, mean = mu.list[[c]], sigma = Sigma.list[[c]])
}
return(sum(log(foo)))
}
# Now I recommend the following
# mu is a list
# Sigma is a list
GMMforD <- function(X, C = 2, tol = 1e-5, maxit = 1e3)
{
n <- dim(X)[1]
p <- dim(X)[2]
######## Starting values ###################
## pi are equally split over C
pi.list <- rep(1/C, C)
mu <- list() # list of all the means
Sigma <- list() # list of all variances
# The means for each C cannot be the same,
# since then the three distributions overlap
# Hence adding random noise to colMeans(X)
# the variance of the noise depends on the components
for(i in 1:C)
{
mu[[i]] <- rnorm(p) + colMeans(X)
10
Sigma[[i]] <- var(X) # same covariance matrix
}
# Choosing good starting values is important since
# The GMM likelihood is not concave, so the algorithm
# may converge to a local optima.
######## EM algorithm steps ###################
iter <- 0
diff <- 100
epsilon <- 1e-05 # postive-def of Sigma
Ep <- matrix(0, nrow = n, ncol = C) # gamma_{i,c}
current <- c(unlist(mu), unlist(Sigma), pi.list)
while((diff > tol) && (iter < maxit) )
{
iter <- iter + 1
previous <- current
## E step: find gammas
for(c in 1:C)
{
Ep[ ,c] <- pi.list[c]*apply(X, 1, dmvnorm , mu[[c]], Sigma[[c]])
}
Ep <- Ep/rowSums(Ep)
### M-step
pi.list <- colMeans(Ep)
for(c in 1:C)
{
mu[[c]] <- colSums(Ep[ ,c] * X )/sum(Ep[,c])
}
for(c in 1:C)
{
foo <- 0
for(i in 1:n)
{
foo <- foo + (X[i, ] - mu[[c]]) %*% t(X[i, ] - mu[[c]]) * Ep[i,c]
}
Sigma[[c]] <- foo/sum(Ep[,c])
11
if(min(eigen(Sigma[[c]])$values) <=0)
{
# To ensure the estimator is positive definite
# otherwise next iteration gamma_i,c,k cannot be calculated
Sigma[[c]] <- Sigma[[c]] + diag(epsilon, p)
print("Matrix not positive-definite")
}
}
# Difference in the log-likelihoods as the difference criterion
current <- c(unlist(mu), unlist(Sigma), pi.list)
diff <- norm(previous - current, "2")
}
# Final allocation updates
for(c in 1:C)
{
Ep[ ,c] <- pi.list[c]*apply(X, 1, dmvnorm , mu[[c]], Sigma[[c]])
}
Ep <- Ep/rowSums(Ep)
# calculate the loglikelihood of the final est
save.loglike <- log_like(X = X, pi.list = pi.list, mu.list = mu, Sigma.list = Sigma, C = C)
return(list("pi" = pi.list, "mu" = mu, "Sigma" = Sigma, "Ep" = Ep,
"log.like" = save.loglike))
}
X = data(faithful)
summary(X)
X
X = faitrhful
X = faithful
summary(X)
GMM(X)
GMMforD(X)
GMMforD(X,C=2,tol=1e-5,maxit=1e3)
n <- dim(X)[1]
p <- dim(X)[2]
######## Starting values ###################
## pi are equally split over C
pi.list <- rep(1/C, C)
C=2
######## Starting values ###################
## pi are equally split over C
pi.list <- rep(1/C, C)
mu <- list() # list of all the means
Sigma <- list() # list of all variances
# The means for each C cannot be the same,
# since then the three distributions overlap
# Hence adding random noise to colMeans(X)
# the variance of the noise depends on the components
for(i in 1:C)
{
mu[[i]] <- rnorm(p) + colMeans(X)
10
Sigma[[i]] <- var(X) # same covariance matrix
}
mu
Sigma
# Choosing good starting values is important since
# The GMM likelihood is not concave, so the algorithm
# may converge to a local optima.
######## EM algorithm steps ###################
iter <- 0
diff <- 100
epsilon <- 1e-05 # postive-def of Sigma
Ep <- matrix(0, nrow = n, ncol = C) # gamma_{i,c}
current <- c(unlist(mu), unlist(Sigma), pi.list)
current
unlist(mu)
while((diff > tol) && (iter < maxit) )
{
iter <- iter + 1
previous <- current
## E step: find gammas
for(c in 1:C)
{
Ep[ ,c] <- pi.list[c]*apply(X, 1, dmvnorm , mu[[c]], Sigma[[c]])
}
Ep <- Ep/rowSums(Ep)
### M-step
pi.list <- colMeans(Ep)
for(c in 1:C)
{
mu[[c]] <- colSums(Ep[ ,c] * X )/sum(Ep[,c])
}
for(c in 1:C)
{
foo <- 0
for(i in 1:n)
{
foo <- foo + (X[i, ] - mu[[c]]) %*% t(X[i, ] - mu[[c]]) * Ep[i,c]
}
Sigma[[c]] <- foo/sum(Ep[,c])
11
if(min(eigen(Sigma[[c]])$values) <=0)
{
# To ensure the estimator is positive definite
# otherwise next iteration gamma_i,c,k cannot be calculated
Sigma[[c]] <- Sigma[[c]] + diag(epsilon, p)
print("Matrix not positive-definite")
}
}
# Difference in the log-likelihoods as the difference criterion
current <- c(unlist(mu), unlist(Sigma), pi.list)
diff <- norm(previous - current, "2")
}
maxit =1e3
tol=1e-8
while((diff > tol) && (iter < maxit) )
{
iter <- iter + 1
previous <- current
## E step: find gammas
for(c in 1:C)
{
Ep[ ,c] <- pi.list[c]*apply(X, 1, dmvnorm , mu[[c]], Sigma[[c]])
}
Ep <- Ep/rowSums(Ep)
### M-step
pi.list <- colMeans(Ep)
for(c in 1:C)
{
mu[[c]] <- colSums(Ep[ ,c] * X )/sum(Ep[,c])
}
for(c in 1:C)
{
foo <- 0
for(i in 1:n)
{
foo <- foo + (X[i, ] - mu[[c]]) %*% t(X[i, ] - mu[[c]]) * Ep[i,c]
}
Sigma[[c]] <- foo/sum(Ep[,c])
11
if(min(eigen(Sigma[[c]])$values) <=0)
{
# To ensure the estimator is positive definite
# otherwise next iteration gamma_i,c,k cannot be calculated
Sigma[[c]] <- Sigma[[c]] + diag(epsilon, p)
print("Matrix not positive-definite")
}
}
# Difference in the log-likelihoods as the difference criterion
current <- c(unlist(mu), unlist(Sigma), pi.list)
diff <- norm(previous - current, "2")
}
## E step: find gammas
for(c in 1:C)
{
Ep[ ,c] <- pi.list[c]*apply(X, 1, dmvnorm , mu[[c]], Sigma[[c]])
}
Ep
Ep <- Ep/rowSums(Ep)
### M-step
pi.list <- colMeans(Ep)
pi.list
for(c in 1:C)
{
mu[[c]] <- colSums(Ep[ ,c] * X )/sum(Ep[,c])
}
for(c in 1:C)
{
foo <- 0
for(i in 1:n)
{
foo <- foo + (X[i, ] - mu[[c]]) %*% t(X[i, ] - mu[[c]]) * Ep[i,c]
}
Sigma[[c]] <- foo/sum(Ep[,c])
11
if(min(eigen(Sigma[[c]])$values) <=0)
{
# To ensure the estimator is positive definite
# otherwise next iteration gamma_i,c,k cannot be calculated
Sigma[[c]] <- Sigma[[c]] + diag(epsilon, p)
print("Matrix not positive-definite")
}
}
mu[[c]]
X[i, ] - mu[[c]]
foo <- 0
for(i in 1:n)
{
foo <- foo + (X[i, ] - mu[[c]]) %*% t(X[i, ] - mu[[c]]) * Ep[i,c]
}
foo <- 0
for(i in 1:n)
{
foo <- foo + (X[i, ] - mu[[c]]) %*% t(X[i, ] - mu[[c]]) * Ep[i,c]
}
t(X[i, ] - mu[[c]])
(X[i, ] - mu[[c]]) %*% t(X[i, ] - mu[[c]])
(X[i, ] - mu[[c]]) %*% t(X[i, ] - mu[[c]])
foo <- 0
for(i in 1:n)
{
foo <- foo + (as.matrix(X[i, ] - mu[[c]])) %*% (as.matrix(t(X[i, ] - mu[[c]]))) * Ep[i,c]
}
(as.matrix(X[i, ] - mu[[c]])) %*% (as.matrix(t(X[i, ] - mu[[c]])))
m=matrix(c(2,3),2)
x = matrix(c(1,2,3,4),2,2)
m*x
Ep[ ,c] * X
m
X
x
typeof(X)
typeof(x)
typeof(Ep)
for(c in 1:C)
{
foo <- 0
for(i in 1:n)
{
foo <- foo + (as.matrix(X[i, ] - mu[[c]])) %*% (as.matrix(t(X[i, ] - mu[[c]]))) * Ep[i,c]
}
Sigma[[c]] <- foo/sum(Ep[,c])
if(min(eigen(Sigma[[c]])$values) <=0)
{
# To ensure the estimator is positive definite
# otherwise next iteration gamma_i,c,k cannot be calculated
Sigma[[c]] <- Sigma[[c]] + diag(epsilon, p)
print("Matrix not positive-definite")
}
}
# function that calculates the
# the log_likelihood
# for this multivariate setup
library(mvtnorm)
log_like <- function(X, pi.list, mu.list, Sigma.list, C)
{
foo <- 0
for(c in 1:C)
{
foo <- foo + pi.list[[c]]*dmvnorm(X, mean = mu.list[[c]], sigma = Sigma.list[[c]])
}
return(sum(log(foo)))
}
# Now I recommend the following
# mu is a list
# Sigma is a list
GMMforD <- function(X, C = 2, tol = 1e-5, maxit = 1e3)
{
n <- dim(X)[1]
p <- dim(X)[2]
######## Starting values ###################
## pi are equally split over C
pi.list <- rep(1/C, C)
mu <- list() # list of all the means
Sigma <- list() # list of all variances
# The means for each C cannot be the same,
# since then the three distributions overlap
# Hence adding random noise to colMeans(X)
# the variance of the noise depends on the components
for(i in 1:C)
{
mu[[i]] <- rnorm(p) + colMeans(X)
Sigma[[i]] <- var(X) # same covariance matrix
}
# Choosing good starting values is important since
# The GMM likelihood is not concave, so the algorithm
# may converge to a local optima.
######## EM algorithm steps ###################
iter <- 0
diff <- 100
epsilon <- 1e-05 # postive-def of Sigma
Ep <- matrix(0, nrow = n, ncol = C) # gamma_{i,c}
current <- c(unlist(mu), unlist(Sigma), pi.list)
while((diff > tol) && (iter < maxit) )
{
iter <- iter + 1
previous <- current
## E step: find gammas
for(c in 1:C)
{
Ep[ ,c] <- pi.list[c]*apply(X, 1, dmvnorm , mu[[c]], Sigma[[c]])
}
Ep <- Ep/rowSums(Ep)
### M-step
pi.list <- colMeans(Ep)
for(c in 1:C)
{
mu[[c]] <- colSums(Ep[ ,c] * X )/sum(Ep[,c])
}
for(c in 1:C)
{
foo <- 0
for(i in 1:n)
{
foo <- foo + (as.matrix(X[i, ] - mu[[c]])) %*% (as.matrix(t(X[i, ] - mu[[c]]))) * Ep[i,c]
}
Sigma[[c]] <- foo/sum(Ep[,c])
if(min(eigen(Sigma[[c]])$values) <=0)
{
# To ensure the estimator is positive definite
# otherwise next iteration gamma_i,c,k cannot be calculated
Sigma[[c]] <- Sigma[[c]] + diag(epsilon, p)
print("Matrix not positive-definite")
}
}
# Difference in the log-likelihoods as the difference criterion
current <- c(unlist(mu), unlist(Sigma), pi.list)
diff <- norm(previous - current, "2")
}
# Final allocation updates
for(c in 1:C)
{
Ep[ ,c] <- pi.list[c]*apply(X, 1, dmvnorm , mu[[c]], Sigma[[c]])
}
Ep <- Ep/rowSums(Ep)
# calculate the loglikelihood of the final est
save.loglike <- log_like(X = X, pi.list = pi.list, mu.list = mu, Sigma.list = Sigma, C = C)
return(list("pi" = pi.list, "mu" = mu, "Sigma" = Sigma, "Ep" = Ep,
"log.like" = save.loglike))
}
X <- as.matrix(faithful)
C <- 2
class2.1 <- GMMforD(X = X, C = C)
class2.2 <- GMMforD(X = X, C = C)
class2.3 <- GMMforD(X = X, C = C)
class2.4 <- GMMforD(X = X, C = C)
class2.1
Kpar <- function(C, p)
{
(C-1) + C*p + p*(p+1)/2*C
}
class3.1 <- GMMforD(X = X, C = 3)
class3.2 <- GMMforD(X = X, C = 3)
class3.3 <- GMMforD(X = X, C = 3)
class3.4 <- GMMforD(X = X, C = 3)
class3.5 <- GMMforD(X = X, C = 3)
class3.6 <- GMMforD(X = X, C = 3)
BIC.2 <- 2*class2.1$log.like - log(n)*Kpar(C = 2, p = 2)
BIC.3 <- 2*class3.4$log.like - log(n)*Kpar(C = 3, p = 2)
#BIC.4 <- 2*class4.5$log.like - log(n)*Kpar(C = 4, p = 2)
c(BIC.2, BIC.3, BIC.4)
#BIC.4 <- 2*class4.5$log.like - log(n)*Kpar(C = 4, p = 2)
c(BIC.2, BIC.3)
?qnorm
qpois(0.05, 15)
ppois(9, 15)
ppois(8
, 15)
?qpois
qpois(1, 15)
qpois(0, 15)
qpois(0.000001, 15)
qpois(0.00000001, 15)
qpois(exp(-15), 15)
qpois(exp(-16), 15)
qpois(exp(-14.99), 15)
qpois(exp(0.05), 15)
qpois((0.05), 15)
ppois(8, 15)
(0.05-ppois(8, 15))/dpois(8, 15)
qpois((0.01), 15)
(0.01-ppois(6, 15))/dpois(6, 15)
qpois((0.1), 15)
\clc
\cls
qnorm(0.9)
qnorm(0.1)
0.8*qnorm(0.9)
qt(0.975,23)
qt(0.95,23)
qt(0.95,23,lower.tail = "TRUE")
qt(0.95,23,lower.tail = TRUE)
qt(0.975,23,lower.tail = TRUE)
install.packages('ROCR')
prediction()
x = c(1.2,3.4,5.6,6.5,4.3)
y = c(2.3,4.5,6.7,8.9,1.2)
m1 = glm(y~x,family = binomial())
x = c(1.2,3.4,5.6,6.5,4.3)
y = c(0,0,1,0,1)
m1 = glm(y~x,family = binomial())
fit = m1$fitted.values
auc = performance(fit,measure='tnr',x.measure = 'tpr')
install.packages('gplots')
library(gplots)
install.packages('ROCR')
library(ROCR)
auc = performance(fit,measure='tnr',x.measure = 'tpr')
pred = prediction(fit,x)
pred = prediction(fit,y)
auc = performance(fit,measure='tnr',x.measure = 'tpr')
auc = performance(pred,measure='tnr',x.measure = 'tpr')
auc@x.values
auc@y.values
xVal = auc@x.values
yVal = auc@y.values
deltaX =xVal[-1] - xVal[-length(xVal)]
plot(auc)
install.packages('RODBC')
library(RODBC)
setwd("C:/Users/HP/OneDrive/Desktop/Spatial Project")
dbname = 'PROTEA_handover_20170214.mdb'
temp = odbcConnect(dbname)
library(maptools)
library(disdat)
library(rgeos)
library(rgdal)
library(raster)
library(dismo)
library(geos)
library(terra)
library(geodata)
library(RODBC)
data('wrld_simpl')
plot(wrld_simpl)
climate = getData('worldclim',var ='bio' ,res = 2.5)
climate = getData('worldclim',var ='bio' ,res = 2.5)
